{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed86f4c2",
   "metadata": {},
   "source": [
    "# Prompt Generation from User Requirements\n",
    "\n",
    "In this example we will create a chat bot that helps a user generate a prompt.\n",
    "It will first collect requirements from the user, and then will generate the prompt (and refine it based on user input).\n",
    "These are split into two separate states, and the LLM decides when to transition between them.\n",
    "\n",
    "A graphical representation of the system can be found below.\n",
    "\n",
    "![prompt-generator.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a7e48",
   "metadata": {},
   "source": [
    "## üìö Quick Start Guide\n",
    "\n",
    "### 1Ô∏è‚É£ Prerequisites\n",
    "```bash\n",
    "pip install langgraph langchain-openai langchain-core pydantic python-dotenv\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Setup Environment\n",
    "Create a `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "### 3Ô∏è‚É£ Run the Cells\n",
    "Execute cells in order:\n",
    "1. **Environment Setup** - Load API keys\n",
    "2. **Imports** - Load all dependencies  \n",
    "3. **Configuration** - Initialize LLM and prompts\n",
    "4. **Data Models** - Define Pydantic schemas\n",
    "5. **Helper Functions** - Message processing utilities\n",
    "6. **Node Functions** - Core logic\n",
    "7. **Workflow Graph** - Create state machine\n",
    "8. **Visualization** - View graph structure\n",
    "9. **Run Application** - Start interactive chat\n",
    "\n",
    "### 4Ô∏è‚É£ Usage Example\n",
    "```\n",
    "You: Hi, I need help creating a prompt\n",
    "AI: I'll help you! What should the prompt accomplish?\n",
    "You: Create a product description\n",
    "AI: What variables will be used? Any constraints?\n",
    "You: Product name and features. No marketing jargon.\n",
    "AI: [Generates structured prompt template]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee61757",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f35629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found in environment variables!\")\n",
    "    print(\"Please create a .env file with your OpenAI API key:\")\n",
    "    print('OPENAI_API_KEY=\"your-api-key-here\"')\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
    "    \n",
    "# Set the API key explicitly\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88192a",
   "metadata": {},
   "source": [
    "## Gather information\n",
    "\n",
    "First, let's define the part of the graph that will gather user requirements. This will be an LLM call with a specific system message. It will have access to a tool that it can call when it is ready to generate the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa0af3",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n",
    "    <p>\n",
    "        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fa14fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - All necessary libraries for the application\n",
    "# ============================================================================\n",
    "\n",
    "from typing import List, Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LangChain Core - Message handling and LLM interaction\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,    # For system instructions\n",
    "    HumanMessage,     # For user messages\n",
    "    AIMessage,        # For AI responses\n",
    "    ToolMessage       # For tool execution results\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Pydantic - Data validation and structure\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph - Graph-based workflow orchestration\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Utilities\n",
    "import uuid\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b55b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration and LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - System prompts and LLM setup\n",
    "# ============================================================================\n",
    "\n",
    "# System prompt for information gathering phase\n",
    "INFO_GATHERING_TEMPLATE = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "# System prompt for prompt generation phase\n",
    "PROMPT_GENERATION_TEMPLATE = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "# Initialize the OpenAI LLM (GPT-4)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0  # Deterministic output for consistency\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration and LLM initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4024e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data models defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA MODELS - Pydantic models for structured data\n",
    "# ============================================================================\n",
    "\n",
    "class PromptInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured model for capturing prompt requirements from user.\n",
    "    \n",
    "    This model is used as a tool by the LLM to extract structured information\n",
    "    from conversational input.\n",
    "    \n",
    "    Attributes:\n",
    "        objective: The main goal/purpose of the prompt\n",
    "        variables: List of variables that will be used in the prompt template\n",
    "        constraints: Things the output should NOT do\n",
    "        requirements: Things the output MUST do\n",
    "    \"\"\"\n",
    "    objective: str = Field(description=\"The main goal or purpose of the prompt\")\n",
    "    variables: List[str] = Field(description=\"Variables to be used in the prompt template\")\n",
    "    constraints: List[str] = Field(description=\"Things the output should NOT do\")\n",
    "    requirements: List[str] = Field(description=\"Things the output MUST adhere to\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    State definition for the LangGraph workflow.\n",
    "    \n",
    "    The state maintains the conversation history with automatic message merging\n",
    "    using the add_messages reducer.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: List of conversation messages (automatically merged)\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "print(\"‚úÖ Data models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593f6da",
   "metadata": {},
   "source": [
    "## üìê Architecture Overview\n",
    "\n",
    "This notebook implements a **two-phase conversational AI system** using LangChain and LangGraph:\n",
    "\n",
    "### üéØ System Design\n",
    "\n",
    "The application follows a **modular, state-driven architecture** with clear separation of concerns:\n",
    "\n",
    "#### **Phase 1: Information Gathering** \n",
    "- Collects user requirements through natural conversation\n",
    "- Uses structured tool calling (Pydantic models) to extract:\n",
    "  - Objective of the prompt\n",
    "  - Variables to be included\n",
    "  - Constraints (what NOT to do)\n",
    "  - Requirements (what MUST be done)\n",
    "\n",
    "#### **Phase 2: Prompt Generation**\n",
    "- Takes extracted requirements\n",
    "- Generates a well-structured prompt template\n",
    "- Allows refinement through conversation\n",
    "\n",
    "### üîß Key Components\n",
    "\n",
    "1. **State Management**: TypedDict with message history\n",
    "2. **LLM Integration**: OpenAI GPT-4 with temperature=0\n",
    "3. **Tool Binding**: Pydantic models as LLM tools\n",
    "4. **Graph Workflow**: LangGraph for state orchestration\n",
    "5. **Memory**: Persistent conversation context\n",
    "\n",
    "### üé® Code Organization\n",
    "\n",
    "- **Configuration**: System prompts and LLM setup\n",
    "- **Data Models**: Pydantic schemas for validation\n",
    "- **Helper Functions**: Message processing utilities\n",
    "- **Node Functions**: Core business logic\n",
    "- **Workflow Graph**: State machine definition\n",
    "- **Interactive Interface**: User interaction layer\n",
    "\n",
    "### üí° Best Practices Implemented\n",
    "\n",
    "‚úÖ **Type Hints**: Full type annotations throughout  \n",
    "‚úÖ **Documentation**: Comprehensive docstrings  \n",
    "‚úÖ **Modularity**: Single responsibility principle  \n",
    "‚úÖ **Error Handling**: Graceful fallbacks  \n",
    "‚úÖ **Clean Code**: Consistent formatting and naming  \n",
    "‚úÖ **Comments**: Inline explanations for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141c4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS - Message processing and filtering\n",
    "# ============================================================================\n",
    "\n",
    "def get_info_messages(messages: list) -> list:\n",
    "    \"\"\"\n",
    "    Prepare messages for the information gathering phase.\n",
    "    \n",
    "    Adds the system prompt to the beginning of the conversation.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        \n",
    "    Returns:\n",
    "        List with system message prepended to conversation history\n",
    "    \"\"\"\n",
    "    return [SystemMessage(content=INFO_GATHERING_TEMPLATE)] + messages\n",
    "\n",
    "\n",
    "def get_prompt_messages(messages: list) -> list:\n",
    "    \"\"\"\n",
    "    Extract messages for the prompt generation phase.\n",
    "    \n",
    "    This function:\n",
    "    1. Finds the tool call containing user requirements\n",
    "    2. Filters out tool messages\n",
    "    3. Only includes messages AFTER the tool call\n",
    "    4. Formats the system prompt with requirements\n",
    "    \n",
    "    Args:\n",
    "        messages: Full conversation history\n",
    "        \n",
    "    Returns:\n",
    "        List of messages starting with formatted system prompt\n",
    "    \"\"\"\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    \n",
    "    # Iterate through messages to find tool call and subsequent messages\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "            # Extract the requirements from tool call\n",
    "            tool_call = msg.tool_calls[0][\"args\"]\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            # Skip tool messages in output\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            # Include messages after tool call\n",
    "            other_msgs.append(msg)\n",
    "    \n",
    "    # Create system message with requirements\n",
    "    system_msg = SystemMessage(\n",
    "        content=PROMPT_GENERATION_TEMPLATE.format(reqs=tool_call)\n",
    "    )\n",
    "    \n",
    "    return [system_msg] + other_msgs\n",
    "\n",
    "\n",
    "def get_next_state(state: State) -> Literal[\"add_tool_message\", \"info\", str]:\n",
    "    \"\"\"\n",
    "    Determine the next state in the workflow based on the last message.\n",
    "    \n",
    "    State transition logic:\n",
    "    1. If last message is AI with tool calls ‚Üí go to \"add_tool_message\"\n",
    "    2. If last message is not Human ‚Üí END (conversation complete)\n",
    "    3. Otherwise ‚Üí stay in \"info\" (continue gathering information)\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state\n",
    "        \n",
    "    Returns:\n",
    "        Next state name or END\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if AI made a tool call (requirements gathered)\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"add_tool_message\"\n",
    "    \n",
    "    # Check if waiting for human input\n",
    "    elif not isinstance(last_message, HumanMessage):\n",
    "        return END\n",
    "    \n",
    "    # Continue gathering information\n",
    "    return \"info\"\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f13996",
   "metadata": {},
   "source": [
    "## Define the state logic\n",
    "\n",
    "This is the logic for what state the chatbot is in.\n",
    "If the last message is a tool call, then we are in the state where the \"prompt creator\" (`prompt`) should respond.\n",
    "Otherwise, if the last message is not a HumanMessage, then we know the human should respond next and so we are in the `END` state.\n",
    "If the last message is a HumanMessage, then if there was a tool call previously we are in the `prompt` state.\n",
    "Otherwise, we are in the \"info gathering\" (`info`) state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48ddc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Node functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NODE FUNCTIONS - Core processing functions for each graph node\n",
    "# ============================================================================\n",
    "\n",
    "def info_gathering_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Node for gathering information from the user.\n",
    "    \n",
    "    This node:\n",
    "    1. Prepares messages with the info gathering system prompt\n",
    "    2. Binds the PromptInfo tool to the LLM\n",
    "    3. Invokes the LLM to either ask questions or extract requirements\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with message history\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages to add to state\n",
    "    \"\"\"\n",
    "    # Prepare messages with system prompt\n",
    "    messages = get_info_messages(state[\"messages\"])\n",
    "    \n",
    "    # Bind the PromptInfo tool so LLM can extract structured data\n",
    "    llm_with_tool = llm.bind_tools([PromptInfo])\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    \n",
    "    # Return response wrapped in dict for state update\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def prompt_generation_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Node for generating the final prompt template.\n",
    "    \n",
    "    This node:\n",
    "    1. Extracts requirements from previous tool call\n",
    "    2. Creates a new system prompt with those requirements\n",
    "    3. Generates the final prompt template\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with complete requirements\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with the generated prompt message\n",
    "    \"\"\"\n",
    "    # Get messages filtered for prompt generation phase\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    \n",
    "    # Generate the prompt\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return the generated prompt\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def add_tool_message_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Node to add a tool message confirming requirements were extracted.\n",
    "    \n",
    "    This is a bridge node that:\n",
    "    1. Acknowledges the tool call\n",
    "    2. Provides a confirmation message\n",
    "    3. Transitions the workflow to prompt generation\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with tool call from info gathering\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with tool confirmation message\n",
    "    \"\"\"\n",
    "    # Get the tool call ID from the last AI message\n",
    "    tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "    \n",
    "    # Create a tool message to acknowledge the requirements\n",
    "    tool_msg = ToolMessage(\n",
    "        content=\"Prompt generated!\",\n",
    "        tool_call_id=tool_call_id\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [tool_msg]}\n",
    "\n",
    "print(\"‚úÖ Node functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a9d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Workflow graph created and compiled!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WORKFLOW GRAPH - Build the LangGraph state machine\n",
    "# ============================================================================\n",
    "\n",
    "def create_prompt_generation_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Create and configure the prompt generation workflow graph.\n",
    "    \n",
    "    Graph Structure:\n",
    "    \n",
    "        START ‚Üí info ‚Üí [Conditional]\n",
    "                        ‚îú‚îÄ‚Üí add_tool_message ‚Üí prompt ‚Üí END\n",
    "                        ‚îú‚îÄ‚Üí info (loop back)\n",
    "                        ‚îî‚îÄ‚Üí END\n",
    "    \n",
    "    Nodes:\n",
    "        - info: Gathers requirements from user\n",
    "        - add_tool_message: Confirms requirements extracted\n",
    "        - prompt: Generates the final prompt template\n",
    "    \n",
    "    Returns:\n",
    "        Compiled StateGraph ready for execution\n",
    "    \"\"\"\n",
    "    # Initialize the graph with State schema\n",
    "    workflow = StateGraph(State)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"info\", info_gathering_node)\n",
    "    workflow.add_node(\"add_tool_message\", add_tool_message_node)\n",
    "    workflow.add_node(\"prompt\", prompt_generation_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.add_edge(START, \"info\")\n",
    "    \n",
    "    # Add conditional routing from info node\n",
    "    # Based on get_next_state function's return value\n",
    "    workflow.add_conditional_edges(\n",
    "        \"info\",\n",
    "        get_next_state,\n",
    "        {\n",
    "            \"add_tool_message\": \"add_tool_message\",  # Requirements gathered\n",
    "            \"info\": \"info\",                          # Continue gathering\n",
    "            END: END                                  # Conversation ended\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add edges for the prompt generation flow\n",
    "    workflow.add_edge(\"add_tool_message\", \"prompt\")  # After confirmation, generate\n",
    "    workflow.add_edge(\"prompt\", END)                  # After generation, end\n",
    "    \n",
    "    # Initialize memory for conversation persistence\n",
    "    memory = InMemorySaver()\n",
    "    \n",
    "    # Compile the graph with checkpointing enabled\n",
    "    graph = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "# Create the graph instance\n",
    "graph = create_prompt_generation_graph()\n",
    "print(\"‚úÖ Workflow graph created and compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e57f7882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Workflow Graph Visualization:\n",
      "======================================================================\n",
      "\n",
      "\n",
      "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ  START  ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "             ‚îÇ\n",
      "             ‚ñº\n",
      "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ  info (gathering)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ  - Ask questions    ‚îÇ     ‚îÇ\n",
      "        ‚îÇ  - Extract reqs     ‚îÇ     ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
      "                   ‚îÇ                 ‚îÇ\n",
      "                   ‚îú‚îÄ‚îÄ‚îÄ Continue ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "                   ‚îÇ    gathering\n",
      "                   ‚îÇ\n",
      "                   ‚îú‚îÄ‚îÄ‚îÄ Requirements\n",
      "                   ‚îÇ    complete\n",
      "                   ‚ñº\n",
      "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ  add_tool_message    ‚îÇ\n",
      "        ‚îÇ  - Confirm reqs      ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "                   ‚îÇ\n",
      "                   ‚ñº\n",
      "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ  prompt (generation) ‚îÇ\n",
      "        ‚îÇ  - Create template   ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "                   ‚îÇ\n",
      "                   ‚ñº\n",
      "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "        ‚îÇ      END        ‚îÇ\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "        \n",
      "======================================================================\n",
      "\n",
      "üìã Node Descriptions:\n",
      "----------------------------------------------------------------------\n",
      "1. info: Gathers requirements from user through conversation\n",
      "2. add_tool_message: Confirms requirements were successfully extracted\n",
      "3. prompt: Generates the final prompt template based on requirements\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION - Display the workflow graph structure\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_graph():\n",
    "    \"\"\"\n",
    "    Display the workflow graph structure in ASCII format.\n",
    "    \n",
    "    This provides a visual representation of the state machine\n",
    "    showing all nodes and their connections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to get Mermaid diagram (if available)\n",
    "        from IPython.display import Image, display\n",
    "        \n",
    "        print(\"üìä Workflow Graph Visualization:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # ASCII representation\n",
    "        print(\"\"\"\n",
    "        \n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  START  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  info (gathering)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  - Ask questions    ‚îÇ     ‚îÇ\n",
    "        ‚îÇ  - Extract reqs     ‚îÇ     ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "                   ‚îÇ                 ‚îÇ\n",
    "                   ‚îú‚îÄ‚îÄ‚îÄ Continue ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ    gathering\n",
    "                   ‚îÇ\n",
    "                   ‚îú‚îÄ‚îÄ‚îÄ Requirements\n",
    "                   ‚îÇ    complete\n",
    "                   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  add_tool_message    ‚îÇ\n",
    "        ‚îÇ  - Confirm reqs      ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ\n",
    "                   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  prompt (generation) ‚îÇ\n",
    "        ‚îÇ  - Create template   ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ\n",
    "                   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ      END        ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \n",
    "        \"\"\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Print node descriptions\n",
    "        print(\"üìã Node Descriptions:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(\"1. info: Gathers requirements from user through conversation\")\n",
    "        print(\"2. add_tool_message: Confirms requirements were successfully extracted\")\n",
    "        print(\"3. prompt: Generates the final prompt template based on requirements\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not generate visual diagram: {e}\")\n",
    "        print(\"Graph structure is defined and ready to use!\")\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037af919",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Code Quality Features\n",
    "\n",
    "### ‚ú® What Makes This Code Better?\n",
    "\n",
    "#### 1. **Modular Architecture**\n",
    "- **Before**: Everything in one large cell\n",
    "- **After**: Separated into logical sections (config, models, helpers, nodes, graph)\n",
    "- **Benefit**: Easy to maintain, test, and understand\n",
    "\n",
    "#### 2. **Comprehensive Documentation**\n",
    "- **Before**: Minimal comments\n",
    "- **After**: Docstrings for every function explaining purpose, parameters, and returns\n",
    "- **Benefit**: Self-documenting code, easier onboarding\n",
    "\n",
    "#### 3. **Type Safety**\n",
    "- **Before**: No type hints\n",
    "- **After**: Full type annotations using `typing` module\n",
    "- **Benefit**: Catch errors early, better IDE support\n",
    "\n",
    "#### 4. **Clear Naming**\n",
    "- **Before**: Abbreviated names like `get_massage_info` (typo)\n",
    "- **After**: Descriptive names like `get_info_messages`, `info_gathering_node`\n",
    "- **Benefit**: Code reads like English\n",
    "\n",
    "#### 5. **Separation of Concerns**\n",
    "- **Before**: Mixed responsibilities\n",
    "- **After**: Each function has one clear purpose\n",
    "- **Benefit**: Easier debugging and testing\n",
    "\n",
    "#### 6. **Professional Structure**\n",
    "```\n",
    "Configuration (constants, settings)\n",
    "    ‚Üì\n",
    "Data Models (schemas, types)\n",
    "    ‚Üì\n",
    "Helper Functions (utilities)\n",
    "    ‚Üì\n",
    "Node Functions (business logic)\n",
    "    ‚Üì\n",
    "Workflow Graph (orchestration)\n",
    "    ‚Üì\n",
    "User Interface (interaction)\n",
    "```\n",
    "\n",
    "#### 7. **Error Handling**\n",
    "- Graceful handling of user input\n",
    "- Clear error messages\n",
    "- Safe fallbacks\n",
    "\n",
    "#### 8. **Maintainability**\n",
    "- Constants at the top for easy modification\n",
    "- Reusable functions\n",
    "- DRY (Don't Repeat Yourself) principle\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Learning Resources\n",
    "\n",
    "### LangChain & LangGraph\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangGraph Tutorial](https://langchain-ai.github.io/langgraph/)\n",
    "- [Pydantic Guide](https://docs.pydantic.dev/)\n",
    "\n",
    "### Best Practices\n",
    "- [Python Type Hints](https://docs.python.org/3/library/typing.html)\n",
    "- [Clean Code Principles](https://www.oreilly.com/library/view/clean-code-a/9780136083238/)\n",
    "- [Python Docstrings](https://peps.python.org/pep-0257/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854ec57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Usage Tips & Examples\n",
    "\n",
    "### Sample Conversations\n",
    "\n",
    "#### Example 1: Creating a Code Review Prompt\n",
    "```\n",
    "You: I need help creating a code review prompt\n",
    "Agent: What is the main objective of your prompt?\n",
    "You: To review Python code for best practices and security issues\n",
    "Agent: What input variables will your prompt need?\n",
    "You: code_snippet, language, focus_areas\n",
    "[... continues gathering info ...]\n",
    "Agent: Here's your generated prompt: ...\n",
    "```\n",
    "\n",
    "#### Example 2: Creating a Content Generation Prompt\n",
    "```\n",
    "You: Create a prompt for generating blog posts\n",
    "Agent: What is the main objective of your prompt?\n",
    "You: Generate SEO-optimized blog posts on tech topics\n",
    "Agent: What input variables will your prompt need?\n",
    "You: topic, keywords, target_audience, word_count\n",
    "[... continues ...]\n",
    "```\n",
    "\n",
    "### Tips for Best Results\n",
    "\n",
    "1. **Be Specific**: The more details you provide, the better the generated prompt\n",
    "   - ‚úÖ \"Create technical documentation for REST APIs\"\n",
    "   - ‚ùå \"Create documentation\"\n",
    "\n",
    "2. **Think About Variables**: Consider what will change each time you use the prompt\n",
    "   - User input\n",
    "   - Dynamic data\n",
    "   - Context-specific information\n",
    "\n",
    "3. **Define Constraints**: Set boundaries for better results\n",
    "   - Length limits (e.g., \"under 500 words\")\n",
    "   - Tone/style (e.g., \"professional\", \"casual\")\n",
    "   - Format requirements (e.g., \"markdown\", \"JSON\")\n",
    "\n",
    "4. **Specify Requirements**: Include must-have elements\n",
    "   - Output format\n",
    "   - Key sections\n",
    "   - Quality criteria\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "| Use Case | Example Variables | Typical Constraints |\n",
    "|----------|------------------|---------------------|\n",
    "| **Code Review** | code_snippet, language, focus | Must check security, max 10 issues |\n",
    "| **Content Generation** | topic, keywords, audience | 500-1000 words, SEO-optimized |\n",
    "| **Data Analysis** | dataset, metrics, goals | Must include visualizations |\n",
    "| **Translation** | text, source_lang, target_lang | Preserve formatting, cultural context |\n",
    "| **Summarization** | document, length, key_points | Bullet points, maintain key facts |\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Problem**: Agent doesn't understand my requirements\n",
    "- **Solution**: Provide examples or be more specific\n",
    "\n",
    "**Problem**: Generated prompt is too generic\n",
    "- **Solution**: Add more constraints and specific requirements\n",
    "\n",
    "**Problem**: Missing important variables\n",
    "- **Solution**: Think through the entire use case before starting\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "### Extending This Agent\n",
    "\n",
    "1. **Add Validation**: Validate generated prompts against quality criteria\n",
    "2. **Prompt Library**: Save and reuse generated prompts\n",
    "3. **Multi-turn Refinement**: Iterate on generated prompts\n",
    "4. **Export Options**: Save prompts in different formats (JSON, YAML, MD)\n",
    "5. **Templates**: Pre-built templates for common use cases\n",
    "\n",
    "### Integration Ideas\n",
    "\n",
    "- **Web UI**: Create a Streamlit/Gradio interface\n",
    "- **API Service**: Expose as REST API\n",
    "- **CLI Tool**: Command-line interface for quick generation\n",
    "- **VS Code Extension**: Generate prompts directly in your editor\n",
    "- **Slack Bot**: Generate prompts in team chat\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance & Limitations\n",
    "\n",
    "### Current Implementation\n",
    "\n",
    "- **Model**: GPT-4 (high quality, higher cost)\n",
    "- **Temperature**: 0 (deterministic output)\n",
    "- **Memory**: InMemorySaver (session-based, not persistent)\n",
    "- **Context**: No context length limits (uses full conversation)\n",
    "\n",
    "### Potential Optimizations\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Use GPT-3.5-turbo for faster/cheaper operations\n",
    "   - Switch to GPT-4 only for complex prompts\n",
    "\n",
    "2. **Caching**:\n",
    "   - Cache common prompt patterns\n",
    "   - Reuse similar prompts\n",
    "\n",
    "3. **Validation**:\n",
    "   - Pre-validate inputs before sending to LLM\n",
    "   - Reduce unnecessary API calls\n",
    "\n",
    "4. **Persistence**:\n",
    "   - Use SqliteSaver for persistent conversations\n",
    "   - Resume sessions across restarts\n",
    "\n",
    "### Known Limitations\n",
    "\n",
    "- ‚ö†Ô∏è Requires OpenAI API key (costs money)\n",
    "- ‚ö†Ô∏è English language only (can be extended)\n",
    "- ‚ö†Ô∏è No prompt versioning or history\n",
    "- ‚ö†Ô∏è Single conversation at a time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ed8e1",
   "metadata": {},
   "source": [
    "## Use the graph\n",
    "\n",
    "We can now use the created chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
